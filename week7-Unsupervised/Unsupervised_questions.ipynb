{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data science libraries\n",
    "import sklearn as sk\n",
    "from sklearn.cluster import KMeans   # KMeans function\n",
    "from sklearn.datasets import make_circles, make_blobs  # Datasets\n",
    "from sklearn.model_selection import train_test_split   # Cross validation library\n",
    "from sklearn import mixture\n",
    "\n",
    "# Data visualization libaries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   # A must! For nice an easy figures - look for sns command in the notebook\n",
    "from matplotlib.pyplot import cm   # This is the color chart that I personnaly prefer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Homemade libraries\n",
    "from utilities import color, super_scat_it, distance, initiate, estimate_centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week \\#7 - Unsupervised Learning - Exercices\n",
    "\n",
    "This tutorial will focus on two tasks which used unsupervised learning, namely clustering (Sections 7.2 and 7.3) and dimensionality reduction (Section 7). The goal of this tutorial is to develop basic intuition of classic algorithms used for unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Data generation\n",
    "\n",
    "Let's first look at [gaussian mixtures](https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model), a relatively simple model, which we will generate with the [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs) function. \n",
    "\n",
    "**Remark**: In order to properly run Section 7.2.1, do not change the attributes of the `make_blobs` function in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 1000   # Number of observation\n",
    "k = 2   # Number of clusters\n",
    "std = 4   # Standard deviation associated to the isotopic Gaussian Mixture - \n",
    "dim = 2   # Covariates dimension - if dim > 2, don't expect data vizualisation from matplotlib!\n",
    "seed = 10   # Random seed to replicate the experience\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std= std, n_features=dim, random_state=seed)   # Data generation\n",
    "\n",
    "super_scat_it(X, y, k)   # Data visualization - see utilities.py script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Clustering: K-means Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Implementation with numpy\n",
    "\n",
    "The objective of this section is to implement and understand the procedures associated with the k-means algorithm. First, we will therefore implement the k-means algorithm with numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7.1**\n",
    "\n",
    "According to the pseudo code presented on [Slide 18 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), complete the `calcul` function of the `k_means` class below.\n",
    "\n",
    "**Answer 7.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means:\n",
    "\n",
    "    def __init__(self, data, k, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: unlabeled data\n",
    "            k: number of cluster\n",
    "        Class Attributes:\n",
    "            self.data: unlabeled data\n",
    "            self.centroid: cluster centers\n",
    "            self.label: label \n",
    "            self.iteration: number of iteration before k-means converges\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data = data\n",
    "        self.centroid = initiate(data, k)\n",
    "        self.label = np.argmin(distance(self.data, self.centroid ), axis=1)\n",
    "            \n",
    "    def calcul(self):\n",
    "        \n",
    "        self.centroid = estimate_centroid(self.data, self.label)\n",
    "        label_new = np.argmin(distance(self.data, self.centroid), axis=1)\n",
    "        \n",
    "        # Complete the loop (steps 1 and 2 of Slide 18)\n",
    "        while # Condition formalizing convergence of k-means!\n",
    "            self.label = label_new\n",
    "            # Step 2\n",
    "            # Step 1\n",
    "            \n",
    "        self.label = label_new\n",
    "        self.objective = np.mean(np.min(distance(self.data, self.centroid), axis=1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call the above `k_means` class, estimate the centroids and visualize the associated clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km = k_means(X, k)\n",
    "km.calcul()\n",
    "\n",
    "super_scat_it(X, km.label, dim, km.centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Exploration of the K-means Algorithm with Scikit Learn\n",
    "\n",
    "Once the algorithm has been coded, we are going to make our life easier and simply use the [Scikit Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) (-_-).  First, let's check that everything is running fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "super_scat_it(X, kmeans.labels_, dim, kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Choosing the optimal number of clusters\n",
    "\n",
    "Until now, we knew the actual number of subpopulations (parameterized by the variable $ k $) associated with the simulated data. On the other hand, with non simulated datasets, the data is only very rarely labeled. It is therefore important to develop methodologies in order to clearly define the number of clusters required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.2**\n",
    "\n",
    "1. Find a simple way to determine the optimal number of clusters.\n",
    "2. Implement it.\n",
    "3. How many clusters would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers 7.2**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.3**\n",
    "\n",
    "   1. Are you disapointed by the behavior of the curve associated to the validation set?\n",
    "   2. Considering the results obtained here, could you imagine a better way to obtain the optimal number of clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercise was based on a relatively simple dataset. Indeed, there was large number of observations ($n = 1000$) for a relatively small variable space ($\\bf{X} \\in \\mathbb{R}^2$) and small number of clusters ($k = 2$). In order to validate the relevance of the cross-validation procedure to fix the number of clusters, let's now simulate a slightly more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 100   # Number of observation\n",
    "k = 10   # Number of clusters\n",
    "std = 4   # Standard deviation associated to the isotopic Gaussian Mixture - \n",
    "dim = 50   # Covariates dimension - if dim > 2, don't expect data vizualisation from matplotlib!\n",
    "seed = 10   # Random seed to replicate the experience\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std= std, n_features=dim, random_state=seed)   # Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look how the loss associated to the validation set behave on a more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "loss_train, loss_valid = [], []\n",
    "\n",
    "max_cluster = 50\n",
    "\n",
    "for k in np.arange(max_cluster)+1:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_train)\n",
    "    loss_train.append(np.mean(np.min(distance(X_train, kmeans.cluster_centers_), axis=1)))\n",
    "    loss_valid.append(np.mean(np.min(distance(X_valid, kmeans.cluster_centers_), axis=1)))\n",
    "\n",
    "plt.plot(np.arange(max_cluster)+1, loss_train, label='Train')\n",
    "plt.plot(np.arange(max_cluster)+1, loss_valid, label='Validation')\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Ghosting the legend\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))\n",
    "leg.get_frame().set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.4**\n",
    "\n",
    "   1. Are you suprised by the behavior of the curve associated to the training set?\n",
    "   2. Are you disapointed by the behavior of the curve associated to the validation set?\n",
    "    \n",
    "**Answers 7.4**\n",
    "\n",
    "   1. No, we are not. That's what exactly what we expected. Ultimately, the curve associated to the training dataset will converge (with respect to the number of clusters) to 0 (make sure understand why).\n",
    "   2. No! You are not! That's what exactly what you expected. According to the above figure, the validation curve find a global minimum for 10 clusters! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Clustering - The Gaussian Mixture Model\n",
    "\n",
    "We will now consider the [Expectation Maximisation algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) (EM) in order to estimate mixtures of Gaussians. As presented on [Slides 26 to 41 of the course](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_unsupervised.pdf), the idea is simply to associate each observation with a probability of belonging to one or the other of the distributions.\n",
    "\n",
    "Coding the EM algorithm in numpy can be a tedious exercise, so we'll just use the [following library](https://scikit-learn.org/0.16/modules/generated/sklearn.mixture.GMM.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_std=4\n",
    "X, y = make_blobs(n_samples=1000, centers=2, cluster_std=cluster_std, n_features=2, random_state=10)   # Data generation\n",
    "\n",
    "nb_components = 2\n",
    "GMM = mixture.GaussianMixture(n_components=nb_components, covariance_type='full')\n",
    "GMM.fit(X)\n",
    "\n",
    "super_scat_it(X, GMM.predict_proba(X), dim=nb_components, clusters_center=0, task='EM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.5**\n",
    "\n",
    "1. After a quick glance at the figure above, what do you notice that is different from the k-means algorithm?\n",
    "2. What would have happened if we had set the parameter associated with the variance of the sub-populations (`cluster_std`) to 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Choosing the optimal number of clusters for a GMM\n",
    "\n",
    "**Questions 7.6**\n",
    "\n",
    "1. How do we define the right number of clusters?\n",
    "2. Are the curves behave as those previously studied (with the k-means algorithm)? Does it make sense?\n",
    "2. Could you imagine another way to find the optimal number of mixtures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, _, _ = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "loss_train, loss_valid = [], []\n",
    "\n",
    "for k in np.arange(max_cluster)+1:\n",
    "    GMM = mixture.GaussianMixture(n_components=k, covariance_type='full')\n",
    "    GMM.fit(X_train)\n",
    "    \n",
    "    loss_train.append(GMM.score(X_train))\n",
    "    loss_valid.append(GMM.score(X_valid))\n",
    "            \n",
    "plt.plot(np.arange(max_cluster)+1, loss_train, label='Train')\n",
    "plt.plot(np.arange(max_cluster)+1, loss_valid, label='Validation')\n",
    "\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Ghosting the legend\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))\n",
    "leg.get_frame().set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Dimensionality Reduction: Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Model\n",
    "\n",
    "Autoencoders (AE) are a class of neural networks that allow unsupervised learning of the latent characteristics of the data being studied. To do this, the AE will attempt to predict, or copy, the input observations using (multiple) hidden layer. In its simplest form, the architecture of an AE can be summarized in the diagram below.\n",
    "\n",
    "![title](./Images/AE.png)\n",
    "\n",
    "Looking more closely, the AE consists of an encoder, the function $h(\\cdot)$ defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    h(\\mathbf{x}) = \\frac{1}{1+ \\exp(-\\mathbf{W} \\mathbf{x})}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This function takes as input the observations and will consist of recoding it as a hidden layer so as to reduce their size (fewer neurons). Afterwards, an encoder defined by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    f(h(\\mathbf{x})) = \\mathbf{W}^\\top h(\\mathbf{x})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "will attempt <i>to reconstruct </i> the input observations from the hidden layer. In this sense, the AE tries to estimate the observations used as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 AEs with Pytorch\n",
    "\n",
    "In order to build an autoencoder, we will use the\n",
    "<a href=\"https://pytorch.org/\">Pytorch</a> library. It provides two extremely interesting features:\n",
    "<ul>\n",
    "<li> Manipulation of tensors (kind of multidimensional matrices) to perform calculations with GPU. </li>\n",
    "<li> Automatic differentiation (!!!) with the <a href=\"http://pytorch.org/docs/master/autograd.html\">autograd class </a> to easily calculate the gradient descent. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "\n",
    "from torch.nn import functional\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 Data simulation\n",
    "\n",
    "Let's first simulate more complex gaussian mixtures (look at the number of clusters and the dimension of the data) in order to understand the behavior of the AEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs = 10**3   # Number of observations\n",
    "k = 4   # Number of clusters\n",
    "std = 1   # Standard deviation associated to the isotopic Gaussian Mixture - \n",
    "dim = 20   # Covariates dimension - if dim > 2, don't expect data vizualisation from matplotlib!\n",
    "\n",
    "X, y = make_blobs(n_samples=nb_obs, centers=k, cluster_std= std, n_features=dim, random_state=10)   # Data generation\n",
    "X_train, X_valid, _, _ = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will work with Pytorch, let's turn the training and validation datasets into tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = torch.from_numpy(X_train), torch.from_numpy(X_valid) \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=64, shuffle=True, num_workers=2)\n",
    "valid_loader = torch.utils.data.DataLoader(X_valid, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! Remark !!** \n",
    "\n",
    "Although the documentation available for Pytorch is detailed (compared to other deep learning libraries), it is easy to get lost. Nevertheless, for this workshop, it is not necessary to enter all the details associated with the different commands. In fact, the key is to understand the ins and outs of the key steps presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 Building the model\n",
    "\n",
    "#### 7.4.4.1 AE initialization\n",
    "\n",
    "First, let's define the autoencoder class using the <a href=\"http://pytorch.org/docs/master/nn.html#module\"> torch.nn</a> module. In PyTorch, any neural network must inherit this class. The autoencoder uses other common classes in Pytorch, such as <a href = \"http://pytorch.org/docs/master/nn.html#torch.nn. Linear \"> torch.nn.Linear (in_features, out_features)</a>. The latter implements a fully connected linear layer (as its name suggests).\n",
    "\n",
    "During the propagation phase, the `forward` function associated with the propagation of the message defines the operations to be performed in order to calculate the elements of the output. This function is essential and must match the initialization of the model in the previous step to allow proper backpropagation. \n",
    "   \n",
    "Note the use of the <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">torch.nn.functional</a> method which define a set of functions which can be applied to the layers of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, inputs, outputs, features, criterion=None, optimizer=None):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "           self: class name\n",
    "           nb_inputs: number of neurons on the input layer\n",
    "           nb_outputs: number of neurons on the output layer\n",
    "           nb_features: number of neurons on the hidden layer\n",
    "           criterion: loss function used for learning  \n",
    "        \"\"\"\n",
    "        \n",
    "        super(AE, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputs, features)\n",
    "        self.fc2 = nn.Linear(features, outputs)\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input layer, here made up of 1682 neurons\n",
    "        Return:\n",
    "            predictions: output layer\n",
    "        \"\"\"\n",
    "    \n",
    "        h1 = torch.sigmoid(self.fc1(x))\n",
    "        return self.fc2(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined:\n",
    "\n",
    "1. The number of neurons input,\n",
    "2. The number of neurons output, which should be the same as the one in input,\n",
    "3. The number of neurons desired in the hidden layer, which should be smaller than the number of neurons in input.\n",
    "\n",
    "**Question 7.7**\n",
    "\n",
    "1. Why should the dimension of the hidden layer be smaller than the dimension of the input layer?\n",
    "\n",
    "We can now properly initialize the AE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_dim = dim\n",
    "hidden_dim = 4\n",
    "\n",
    "ae = AE(inputs_dim, inputs_dim, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4.2 Loss function\n",
    "\n",
    "Loss function plays an important role in the construction of a supervised and unsupervised models. In fact, it is this same loss function that we try to minimize (or maximize is according to) by iteratively adjusting the weights of the AE. Thus, two different loss functions will most likely result in two different models. As usual, Pytorch offers a large amount of <a href=\"http://pytorch.org/docs/master/nn.html#id42\">loss functions</a> that you can explore at your leisure.\n",
    "\n",
    "Since the data follow gaussians distribution, the mean square error (MSE) seems an interesting first option. Since we have encoded the loss function as an attribute of the autoencoder class, we define it with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4.3 Optimizer\n",
    "\n",
    "PyTorch provides several <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">optimization methods</a> more or less derived from the gradient descent via the `torch 'class. optim`. Among these techniques:\n",
    "\n",
    "<ul>\n",
    "<li> SGD (Stochastic Gradient Descent): implementation of SGD.\n",
    "<li> Adam (Adaptive Moment Estimation): variation of the gradient descent method where the learning rate is adjusted for each parameter.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "ae.optimizer = torch.optim.Adam(ae.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4.4 Learning loop\n",
    "\n",
    "The learning loop goes as follow. Note that the parameters are not optimized during the validation phase and can be controlled with the training attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning(model, data, training=True):\n",
    "    \n",
    "    losses = []\n",
    "    for batch_idx, inputs in enumerate(data):\n",
    "        \n",
    "        if training:\n",
    "            model.optimizer.zero_grad()\n",
    "        \n",
    "        targets = inputs.clone()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = model.criterion(outputs, targets.float())\n",
    "        \n",
    "        if training:\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            loss = model.criterion(model(inputs.float()), targets.float())\n",
    "        \n",
    "        losses.append(loss.data.item())   \n",
    "        \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4.4.5 Training\n",
    "\n",
    "The autoencoder and the associated functions now implemented, we can start to train the model. Once again, the goal here is not to tune the parameters so as to obtain the best possible model, but simply to understand the role that they can play according to the model's predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 51   # Number of iterations\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(0, nb_epoch):\n",
    "    \n",
    "    train_losses.append(learning(model=ae, data=train_loader))\n",
    "    valid_losses.append(learning(model=ae, data=valid_loader, training=False))\n",
    "     \n",
    "    if epoch % 5 == 0: \n",
    "        print(\"epoch: \", \"{:3.0f}\".format(epoch), \"   |   train: \", \"{:1.3f}\".format(train_losses[-1]), \\\n",
    "                        '   |   valid: ', \"{:1.3f}\".format(valid_losses[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at how the AE behaves through epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(8,6)})\n",
    "sns.set(font_scale = 1.5)\n",
    "\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(valid_losses, label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "\n",
    "# Ghosting the legend\n",
    "leg = plt.gca().legend(loc='center left', bbox_to_anchor=(1, .85))\n",
    "leg.get_frame().set_alpha(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5 A representation study\n",
    "\n",
    "Now that the AE is trained, we can look at the latent representation given by the hidden layer. Since we want to visualize the hidden states, we can simply compare the two by two representations from a small amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small amout of data\n",
    "it = iter(enumerate(train_loader))   \n",
    "i, x = next(it)\n",
    "\n",
    "# Let's get the hidden representation\n",
    "outputs = torch.sigmoid(ae.fc1(x.float()))   \n",
    "outputs = outputs.detach()\n",
    "\n",
    "# Visualization\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "axs[0, 0].scatter(outputs[:, 1], outputs[:, 0])\n",
    "axs[0, 0].set_title(\"h$_1$ vs h$_2$\")\n",
    "axs[0, 1].scatter(outputs[:, 2], outputs[:, 0])\n",
    "axs[0, 1].set_title(\"h$_1$ vs h$_3$\")\n",
    "axs[0, 2].scatter(outputs[:, 3], outputs[:, 0])\n",
    "axs[0, 2].set_title(\"h$_1$ vs h$_4$\")\n",
    "axs[1, 0].scatter(outputs[:, 2], outputs[:, 1])\n",
    "axs[1, 0].set_title(\"h$_2$ vs h$_3$\")\n",
    "axs[1, 1].scatter(outputs[:, 3], outputs[:, 1])\n",
    "axs[1, 1].set_title(\"h$_2$ vs h$_4$\")\n",
    "axs[1, 2].scatter(outputs[:, 3], outputs[:, 2])\n",
    "axs[1, 2].set_title(\"h$_3$ vs h$_4$\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions 7.8**\n",
    "\n",
    "1. Are the sublots behave as you expected? How?\n",
    "2. Run the same experiment again without changing the hyperparameters. What do you notice? Does the latent representation seem to have changed?\n",
    "3. Would the subplots have been different if we had simulated the data from 2 clusters instead of 4? Try it! \n",
    "4. How could you profit from the information represented in the hidden layers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
